{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8uCEQLS3zZUn"
   },
   "source": [
    "# **Simple Orchard - Sample MAPPO**\n",
    "This notebook is to demonstrate how to connect the following three components:\n",
    " - Jumanji's environments and functionalities\n",
    " - Mava's MAPPO models\n",
    " - Our custom packages of Jumanji & Mava for our Simple Orchard environment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LYmyi-lU3a-b"
   },
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5l-eEkH-2f0D"
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# @title Import required `mava` packages\n",
    "\n",
    "# below installs all the required mava packages\n",
    "# ! pip install git+https://github.com/instadeepai/mava.git@develop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qYtYcjFr8iBy"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 18:45:53.085742: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# below are the needed mava packages\n",
    "from mava.networks.distributions import IdentityTransformation\n",
    "from mava.evaluator import get_eval_fn, make_ff_eval_act_fn\n",
    "from mava.systems.ppo.types import LearnerState, OptStates, Params, PPOTransition\n",
    "from mava.types import (\n",
    "    State,\n",
    "    MarlEnv,\n",
    "    ActorApply,\n",
    "    CriticApply,\n",
    "    ExperimentOutput,\n",
    "    LearnerFn,\n",
    "    Observation,\n",
    "    ObservationGlobalState,\n",
    ")\n",
    "from mava.utils.jax_utils import (\n",
    "    merge_leading_dims,\n",
    "    unreplicate_batch_dim,\n",
    "    unreplicate_n_dims,\n",
    ")\n",
    "from mava.utils.training import make_learning_rate\n",
    "from mava.wrappers import (\n",
    "    AgentIDWrapper,\n",
    "    RecordEpisodeMetrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "gEq_vdok9F2p"
   },
   "outputs": [],
   "source": [
    "# @title Import required `jumanji` packages\n",
    "\n",
    "# jumanji Env requirements\n",
    "import jumanji\n",
    "from jumanji import register\n",
    "from jumanji.types import TimeStep\n",
    "from jumanji.wrappers import Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pIr2w5FV1Jzb",
    "outputId": "9a5301c7-47dd-4ad4-afed-379dae598059"
   },
   "outputs": [],
   "source": [
    "# # @title Import our required `applesauce` packages\n",
    "\n",
    "# # Check if the applesauce directory exists and delete it if so\n",
    "# import shutil\n",
    "# import os\n",
    "\n",
    "# repo_path = '/content/applesauce'\n",
    "# if os.path.exists(repo_path):\n",
    "#     shutil.rmtree(repo_path)  # Remove the existing directory\n",
    "\n",
    "# # cloning the applesauce repo (erik's branch until merged)\n",
    "# !git clone --branch erik https://github.com/riverliway/applesauce.git\n",
    "\n",
    "# # Specify the path to the existing repository in Colab\n",
    "# import sys\n",
    "# sys.path.append('/content/applesauce/python/jumanji/environments/simple_orchard/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "kMnB3CIW91H6"
   },
   "outputs": [],
   "source": [
    "#importing our custom jumanji (and mava) packages\n",
    "from jumanji_env.environments.simple_orchard.env import SimpleOrchard\n",
    "from jumanji_env.environments.simple_orchard.orchard_types import SimpleOrchardObservation, SimpleOrchardState, SimpleOrchardEntity\n",
    "from jumanji_env.environments.simple_orchard.generator import SimpleOrchardGenerator\n",
    "from jumanji_env.environments.simple_orchard.custom_mava import make_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "FjXA8JyI1_YW"
   },
   "outputs": [],
   "source": [
    "# @title Import remaining required packages.\n",
    "\n",
    "import time\n",
    "from typing import Any, Sequence, Tuple, Dict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import chex\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import tree\n",
    "\n",
    "# Plot requirements\n",
    "import matplotlib.pyplot as plt\n",
    "import optax\n",
    "import tensorflow_probability.substrates.jax.distributions as tfd\n",
    "from colorama import Fore, Style\n",
    "from flax.core.frozen_dict import FrozenDict\n",
    "from flax.linen.initializers import orthogonal\n",
    "from IPython.display import clear_output\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from optax._src.base import OptState\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "sns.set_style(\"white\")\n",
    "sns.color_palette(\"colorblind\")\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CudaDevice(id=0)]\n"
     ]
    }
   ],
   "source": [
    "print(jax.devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9omksZSH6htZ"
   },
   "source": [
    "# Trainer\n",
    "This section encompasses the foundational methods required to set up the training process for MAPPO.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JaIw_5YaUSAB"
   },
   "source": [
    "### Network\n",
    "\n",
    "Initially, we start by constructing the Actor and Critic networks using components from the Flax library.\n",
    "\n",
    "* The `Actor()` network takes an observation as input and produces logits representing the probabilities of different actions. The shapes within the network are determined dynamically based on the number of agents, the observation, and the batch size.\n",
    "* The `Critic()` network takes the global state as input and produces the estimated value of the state. Similar to the Actor network, the shapes within the network are handled implicitly by Flax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Sss6opmC6lmp"
   },
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor Network.\"\"\"\n",
    "\n",
    "    action_dim: Sequence[int]\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, observation: Observation) -> tfd.TransformedDistribution:\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        x = observation.agents_view\n",
    "\n",
    "        actor_output = nn.Dense(128, kernel_init=orthogonal(np.sqrt(2)))(x)\n",
    "        actor_output = nn.relu(actor_output)\n",
    "        actor_output = nn.Dense(128, kernel_init=orthogonal(np.sqrt(2)))(actor_output)\n",
    "        actor_output = nn.relu(actor_output)\n",
    "        actor_output = nn.Dense(self.action_dim, kernel_init=orthogonal(0.01))(actor_output)\n",
    "\n",
    "        masked_logits = jnp.where(\n",
    "            observation.action_mask,\n",
    "            actor_output,\n",
    "            jnp.finfo(jnp.float32).min,\n",
    "        )\n",
    "\n",
    "        return IdentityTransformation(distribution=tfd.Categorical(logits=masked_logits))\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic Network.\"\"\"\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, observation: ObservationGlobalState) -> chex.Array:\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        critic_output = nn.Dense(128, kernel_init=orthogonal(np.sqrt(2)))(observation.agents_view)\n",
    "        critic_output = nn.relu(critic_output)\n",
    "        critic_output = nn.Dense(128, kernel_init=orthogonal(np.sqrt(2)))(critic_output)\n",
    "        critic_output = nn.relu(critic_output)\n",
    "        critic_output = nn.Dense(1, kernel_init=orthogonal(1.0))(critic_output)\n",
    "\n",
    "        return jnp.squeeze(critic_output, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IFraNFqY6s7_"
   },
   "source": [
    "### Learner Function\n",
    "The `get_learner_fn` function returns a function which produces an `ExperimentOutput`, encapsulating the updated learner state, episode information, and loss metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "nZabuAgB_VI0"
   },
   "outputs": [],
   "source": [
    "# below would be the import from mava for the mappo models\n",
    "# however it those models will not work with the above defined networks. So further\n",
    "# study is necessary abstract this out.\n",
    "\n",
    "\n",
    "# from mava.systems.ppo.anakin.ff_mappo import get_learner_fn, learner_setup, run_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "4VVjKmgW64Ct"
   },
   "outputs": [],
   "source": [
    "def get_learner_fn(\n",
    "    env: jumanji.Environment,\n",
    "    apply_fns: Tuple[ActorApply, CriticApply],\n",
    "    update_fns: Tuple[optax.TransformUpdateFn, optax.TransformUpdateFn],\n",
    "    config: DictConfig,\n",
    ") -> LearnerFn[LearnerState]:\n",
    "    \"\"\"Get the learner function.\"\"\"\n",
    "    # Unpack apply and update functions.\n",
    "    actor_apply_fn, critic_apply_fn = apply_fns\n",
    "    actor_update_fn, critic_update_fn = update_fns\n",
    "\n",
    "    def _update_step(learner_state: LearnerState, _: Any) -> Tuple[LearnerState, Tuple]:\n",
    "        \"\"\"A single update of the network.\n",
    "\n",
    "        This function steps the environment and records the trajectory batch for\n",
    "        training. It then calculates advantages and targets based on the recorded\n",
    "        trajectory and updates the actor and critic networks based on the calculated\n",
    "        losses.\n",
    "\n",
    "        Args:\n",
    "        ----\n",
    "            learner_state (NamedTuple):\n",
    "                - params (Params): The current model parameters.\n",
    "                - opt_states (OptStates): The current optimizer states.\n",
    "                - key (PRNGKey): The random number generator state.\n",
    "                - env_state (State): The environment state.\n",
    "                - last_timestep (TimeStep): The last timestep in the current trajectory.\n",
    "            _ (Any): The current metrics info.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        def _env_step(learner_state: LearnerState, _: Any) -> Tuple[LearnerState, PPOTransition]:\n",
    "            \"\"\"Step the environment.\"\"\"\n",
    "            params, opt_states, key, env_state, last_timestep = learner_state\n",
    "\n",
    "            # SELECT ACTION\n",
    "            key, policy_key = jax.random.split(key)\n",
    "            actor_policy = actor_apply_fn(params.actor_params, last_timestep.observation)\n",
    "            value = critic_apply_fn(params.critic_params, last_timestep.observation)\n",
    "            action = actor_policy.sample(seed=policy_key)\n",
    "            log_prob = actor_policy.log_prob(action)\n",
    "\n",
    "            # STEP ENVIRONMENT\n",
    "            env_state, timestep = jax.vmap(env.step, in_axes=(0, 0))(env_state, action)\n",
    "\n",
    "            # LOG EPISODE METRICS\n",
    "            done = tree.map(\n",
    "                lambda x: jnp.repeat(x, config.system.num_agents).reshape(config.arch.num_envs, -1),\n",
    "                timestep.last(),\n",
    "            )\n",
    "\n",
    "            info = timestep.extras[\"episode_metrics\"]\n",
    "\n",
    "            transition = PPOTransition(\n",
    "                done,\n",
    "                action,\n",
    "                value,\n",
    "                timestep.reward,\n",
    "                log_prob,\n",
    "                last_timestep.observation,\n",
    "                info,\n",
    "            )\n",
    "            print(\"Transition:\", transition)\n",
    "            learner_state = LearnerState(params, opt_states, key, env_state, timestep)\n",
    "            return learner_state, transition\n",
    "\n",
    "        # STEP ENVIRONMENT FOR ROLLOUT LENGTH\n",
    "        learner_state, traj_batch = jax.lax.scan(\n",
    "            _env_step, learner_state, None, config.system.rollout_length\n",
    "        )\n",
    "\n",
    "        # CALCULATE ADVANTAGE\n",
    "        params, opt_states, key, env_state, last_timestep = learner_state\n",
    "        last_val = critic_apply_fn(params.critic_params, last_timestep.observation)\n",
    "\n",
    "        def _calculate_gae(\n",
    "            traj_batch: PPOTransition, last_val: chex.Array\n",
    "        ) -> Tuple[chex.Array, chex.Array]:\n",
    "            \"\"\"Calculate the GAE.\"\"\"\n",
    "\n",
    "            def _get_advantages(gae_and_next_value: Tuple, transition: PPOTransition) -> Tuple:\n",
    "                \"\"\"Calculate the GAE for a single transition.\"\"\"\n",
    "                gae, next_value = gae_and_next_value\n",
    "                done, value, reward = (\n",
    "                    transition.done,\n",
    "                    transition.value,\n",
    "                    transition.reward,\n",
    "                )\n",
    "                gamma = config.system.gamma\n",
    "                delta = reward + gamma * next_value * (1 - done) - value\n",
    "                gae = delta + gamma * config.system.gae_lambda * (1 - done) * gae\n",
    "                return (gae, value), gae\n",
    "\n",
    "            _, advantages = jax.lax.scan(\n",
    "                _get_advantages,\n",
    "                (jnp.zeros_like(last_val), last_val),\n",
    "                traj_batch,\n",
    "                reverse=True,\n",
    "                unroll=16,\n",
    "            )\n",
    "            return advantages, advantages + traj_batch.value\n",
    "\n",
    "        advantages, targets = _calculate_gae(traj_batch, last_val)\n",
    "\n",
    "        def _update_epoch(update_state: Tuple, _: Any) -> Tuple:\n",
    "            \"\"\"Update the network for a single epoch.\"\"\"\n",
    "\n",
    "            def _update_minibatch(train_state: Tuple, batch_info: Tuple) -> Tuple:\n",
    "                \"\"\"Update the network for a single minibatch.\"\"\"\n",
    "                # UNPACK TRAIN STATE AND BATCH INFO\n",
    "                params, opt_states, key = train_state\n",
    "                traj_batch, advantages, targets = batch_info\n",
    "\n",
    "                def _actor_loss_fn(\n",
    "                    actor_params: FrozenDict,\n",
    "                    actor_opt_state: OptState,\n",
    "                    traj_batch: PPOTransition,\n",
    "                    gae: chex.Array,\n",
    "                    key: chex.PRNGKey,\n",
    "                ) -> Tuple:\n",
    "                    \"\"\"Calculate the actor loss.\"\"\"\n",
    "                    # RERUN NETWORK\n",
    "                    actor_policy = actor_apply_fn(actor_params, traj_batch.obs)\n",
    "                    log_prob = actor_policy.log_prob(traj_batch.action)\n",
    "\n",
    "                    # CALCULATE ACTOR LOSS\n",
    "                    ratio = jnp.exp(log_prob - traj_batch.log_prob)\n",
    "                    gae = (gae - gae.mean()) / (gae.std() + 1e-8)\n",
    "                    loss_actor1 = ratio * gae\n",
    "                    loss_actor2 = (\n",
    "                        jnp.clip(\n",
    "                            ratio,\n",
    "                            1.0 - config.system.clip_eps,\n",
    "                            1.0 + config.system.clip_eps,\n",
    "                        )\n",
    "                        * gae\n",
    "                    )\n",
    "                    loss_actor = -jnp.minimum(loss_actor1, loss_actor2)\n",
    "                    loss_actor = loss_actor.mean()\n",
    "                    # The seed will be used in the TanhTransformedDistribution:\n",
    "                    entropy = actor_policy.entropy(seed=key).mean()\n",
    "\n",
    "                    total_loss_actor = loss_actor - config.system.ent_coef * entropy\n",
    "                    return total_loss_actor, (loss_actor, entropy)\n",
    "\n",
    "                def _critic_loss_fn(\n",
    "                    critic_params: FrozenDict,\n",
    "                    critic_opt_state: OptState,\n",
    "                    traj_batch: PPOTransition,\n",
    "                    targets: chex.Array,\n",
    "                ) -> Tuple:\n",
    "                    \"\"\"Calculate the critic loss.\"\"\"\n",
    "                    # RERUN NETWORK\n",
    "                    value = critic_apply_fn(critic_params, traj_batch.obs)\n",
    "\n",
    "                    # CALCULATE VALUE LOSS\n",
    "                    value_pred_clipped = traj_batch.value + (value - traj_batch.value).clip(\n",
    "                        -config.system.clip_eps, config.system.clip_eps\n",
    "                    )\n",
    "                    value_losses = jnp.square(value - targets)\n",
    "                    value_losses_clipped = jnp.square(value_pred_clipped - targets)\n",
    "                    value_loss = 0.5 * jnp.maximum(value_losses, value_losses_clipped).mean()\n",
    "\n",
    "                    critic_total_loss = config.system.vf_coef * value_loss\n",
    "                    return critic_total_loss, (value_loss)\n",
    "\n",
    "                # CALCULATE ACTOR LOSS\n",
    "                key, entropy_key = jax.random.split(key)\n",
    "                actor_grad_fn = jax.value_and_grad(_actor_loss_fn, has_aux=True)\n",
    "                actor_loss_info, actor_grads = actor_grad_fn(\n",
    "                    params.actor_params,\n",
    "                    opt_states.actor_opt_state,\n",
    "                    traj_batch,\n",
    "                    advantages,\n",
    "                    entropy_key,\n",
    "                )\n",
    "\n",
    "                # CALCULATE CRITIC LOSS\n",
    "                critic_grad_fn = jax.value_and_grad(_critic_loss_fn, has_aux=True)\n",
    "                critic_loss_info, critic_grads = critic_grad_fn(\n",
    "                    params.critic_params,\n",
    "                    opt_states.critic_opt_state,\n",
    "                    traj_batch,\n",
    "                    targets,\n",
    "                )\n",
    "\n",
    "                # Compute the parallel mean (pmean) over the batch.\n",
    "                # This calculation is inspired by the Anakin architecture demo notebook.\n",
    "                # available at https://tinyurl.com/26tdzs5x\n",
    "                # This pmean could be a regular mean as the batch axis is on the same device.\n",
    "                actor_grads, actor_loss_info = jax.lax.pmean(\n",
    "                    (actor_grads, actor_loss_info), axis_name=\"batch\"\n",
    "                )\n",
    "                # pmean over devices.\n",
    "                actor_grads, actor_loss_info = jax.lax.pmean(\n",
    "                    (actor_grads, actor_loss_info), axis_name=\"device\"\n",
    "                )\n",
    "\n",
    "                critic_grads, critic_loss_info = jax.lax.pmean(\n",
    "                    (critic_grads, critic_loss_info), axis_name=\"batch\"\n",
    "                )\n",
    "                # pmean over devices.\n",
    "                critic_grads, critic_loss_info = jax.lax.pmean(\n",
    "                    (critic_grads, critic_loss_info), axis_name=\"device\"\n",
    "                )\n",
    "\n",
    "                # UPDATE ACTOR PARAMS AND OPTIMISER STATE\n",
    "                actor_updates, actor_new_opt_state = actor_update_fn(\n",
    "                    actor_grads, opt_states.actor_opt_state\n",
    "                )\n",
    "                actor_new_params = optax.apply_updates(params.actor_params, actor_updates)\n",
    "\n",
    "                # UPDATE CRITIC PARAMS AND OPTIMISER STATE\n",
    "                critic_updates, critic_new_opt_state = critic_update_fn(\n",
    "                    critic_grads, opt_states.critic_opt_state\n",
    "                )\n",
    "                critic_new_params = optax.apply_updates(params.critic_params, critic_updates)\n",
    "\n",
    "                new_params = Params(actor_new_params, critic_new_params)\n",
    "                new_opt_state = OptStates(actor_new_opt_state, critic_new_opt_state)\n",
    "\n",
    "                # PACK LOSS INFO\n",
    "                total_loss = actor_loss_info[0] + critic_loss_info[0]\n",
    "                value_loss = critic_loss_info[1]\n",
    "                actor_loss = actor_loss_info[1][0]\n",
    "                entropy = actor_loss_info[1][1]\n",
    "                loss_info = {\n",
    "                    \"total_loss\": total_loss,\n",
    "                    \"value_loss\": value_loss,\n",
    "                    \"actor_loss\": actor_loss,\n",
    "                    \"entropy\": entropy,\n",
    "                }\n",
    "                return (new_params, new_opt_state, entropy_key), loss_info\n",
    "\n",
    "            params, opt_states, traj_batch, advantages, targets, key = update_state\n",
    "            key, shuffle_key, entropy_key = jax.random.split(key, 3)\n",
    "\n",
    "            # SHUFFLE MINIBATCHES\n",
    "            batch_size = config.system.rollout_length * config.arch.num_envs\n",
    "            permutation = jax.random.permutation(shuffle_key, batch_size)\n",
    "            batch = (traj_batch, advantages, targets)\n",
    "            batch = tree.map(lambda x: merge_leading_dims(x, 2), batch)\n",
    "            shuffled_batch = tree.map(lambda x: jnp.take(x, permutation, axis=0), batch)\n",
    "            minibatches = tree.map(\n",
    "                lambda x: jnp.reshape(x, [config.system.num_minibatches, -1] + list(x.shape[1:])),\n",
    "                shuffled_batch,\n",
    "            )\n",
    "\n",
    "            # UPDATE MINIBATCHES\n",
    "            (params, opt_states, entropy_key), loss_info = jax.lax.scan(\n",
    "                _update_minibatch, (params, opt_states, entropy_key), minibatches\n",
    "            )\n",
    "\n",
    "            update_state = (params, opt_states, traj_batch, advantages, targets, key)\n",
    "            return update_state, loss_info\n",
    "\n",
    "        update_state = (params, opt_states, traj_batch, advantages, targets, key)\n",
    "\n",
    "        # UPDATE EPOCHS\n",
    "        update_state, loss_info = jax.lax.scan(\n",
    "            _update_epoch, update_state, None, config.system.ppo_epochs\n",
    "        )\n",
    "\n",
    "        params, opt_states, traj_batch, advantages, targets, key = update_state\n",
    "        learner_state = LearnerState(params, opt_states, key, env_state, last_timestep)\n",
    "        metric = traj_batch.info\n",
    "        return learner_state, (metric, loss_info)\n",
    "\n",
    "    def learner_fn(learner_state: LearnerState) -> ExperimentOutput[LearnerState]:\n",
    "        \"\"\"Learner function.\n",
    "\n",
    "        This function represents the learner, it updates the network parameters\n",
    "        by iteratively applying the `_update_step` function for a fixed number of\n",
    "        updates. The `_update_step` function is vectorized over a batch of inputs.\n",
    "\n",
    "        Args:\n",
    "        ----\n",
    "            learner_state (NamedTuple):\n",
    "                - params (Params): The initial model parameters.\n",
    "                - opt_states (OptStates): The initial optimizer states.\n",
    "                - key (chex.PRNGKey): The random number generator state.\n",
    "                - env_state (LogEnvState): The environment state.\n",
    "                - timesteps (TimeStep): The initial timestep in the initial trajectory.\n",
    "\n",
    "        \"\"\"\n",
    "        batched_update_step = jax.vmap(_update_step, in_axes=(0, None), axis_name=\"batch\")\n",
    "\n",
    "        learner_state, (episode_info, loss_info) = jax.lax.scan(\n",
    "            batched_update_step, learner_state, None, config.system.num_updates_per_eval\n",
    "        )\n",
    "        return ExperimentOutput(\n",
    "            learner_state=learner_state,\n",
    "            episode_metrics=episode_info,\n",
    "            train_metrics=loss_info,\n",
    "        )\n",
    "\n",
    "    return learner_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4idyWUhW68oS"
   },
   "source": [
    "### Learner Setup\n",
    "The learner setup initialises components for training: the learner function, actor and critic networks and optimizers and environment states. It creates a function for learning, employs parallel processing over the cores for efficiency, and sets up initial states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "eWjNSGvZ7ALw"
   },
   "outputs": [],
   "source": [
    "def learner_setup(\n",
    "    env: jumanji.Environment, keys: chex.Array, config: DictConfig\n",
    ") -> Tuple[LearnerFn[LearnerState], Actor, LearnerState]:\n",
    "    \"\"\"Initialise learner_fn, network, optimiser, environment and states.\"\"\"\n",
    "    # Get available TPU cores.\n",
    "    n_devices = len(jax.devices())\n",
    "\n",
    "    # Get number of agents.\n",
    "    config.system.num_agents = env.num_bots\n",
    "\n",
    "    # PRNG keys.\n",
    "    key, actor_net_key, critic_net_key = keys\n",
    "\n",
    "    # Define network and optimiser.\n",
    "    actor_network = Actor(action_dim=6)\n",
    "    critic_network = Critic()\n",
    "\n",
    "    actor_lr = make_learning_rate(config.system.actor_lr, config)\n",
    "    critic_lr = make_learning_rate(config.system.critic_lr, config)\n",
    "\n",
    "    actor_optim = optax.chain(\n",
    "        optax.clip_by_global_norm(config.system.max_grad_norm),\n",
    "        optax.adam(actor_lr, eps=1e-5),\n",
    "    )\n",
    "    critic_optim = optax.chain(\n",
    "        optax.clip_by_global_norm(config.system.max_grad_norm),\n",
    "        optax.adam(critic_lr, eps=1e-5),\n",
    "    )\n",
    "\n",
    "    # Initialise observation with obs of all agents.\n",
    "    obs = env.observation_spec().generate_value()\n",
    "    init_x = tree.map(lambda x: x[jnp.newaxis, ...], obs)\n",
    "\n",
    "    # Initialise actor params and optimiser state.\n",
    "    actor_params = actor_network.init(actor_net_key, init_x)\n",
    "    actor_opt_state = actor_optim.init(actor_params)\n",
    "\n",
    "    # Initialise critic params and optimiser state.\n",
    "    critic_params = critic_network.init(critic_net_key, init_x)\n",
    "    critic_opt_state = critic_optim.init(critic_params)\n",
    "\n",
    "    # Pack params.\n",
    "    params = Params(actor_params, critic_params)\n",
    "\n",
    "    # Pack apply and update functions.\n",
    "    apply_fns = (actor_network.apply, critic_network.apply)\n",
    "    update_fns = (actor_optim.update, critic_optim.update)\n",
    "\n",
    "    # Get batched iterated update and replicate it to pmap it over cores.\n",
    "    learn = get_learner_fn(env, apply_fns, update_fns, config)\n",
    "    learn = jax.pmap(learn, axis_name=\"device\")\n",
    "\n",
    "    # Initialise environment states and timesteps: across devices and batches.\n",
    "    key, *env_keys = jax.random.split(\n",
    "        key, n_devices * config.system.update_batch_size * config.arch.num_envs + 1\n",
    "    )\n",
    "    env_states, timesteps = jax.vmap(env.reset, in_axes=(0))(\n",
    "        jnp.stack(env_keys),\n",
    "    )\n",
    "    reshape_states = lambda x: x.reshape(\n",
    "        (n_devices, config.system.update_batch_size, config.arch.num_envs) + x.shape[1:]\n",
    "    )\n",
    "    # (devices, update batch size, num_envs, ...)\n",
    "    env_states = jax.tree.map(reshape_states, env_states)\n",
    "    timesteps = jax.tree.map(reshape_states, timesteps)\n",
    "\n",
    "    # Define params to be replicated across devices and batches.\n",
    "    key, step_keys = jax.random.split(key)\n",
    "    opt_states = OptStates(actor_opt_state, critic_opt_state)\n",
    "    replicate_learner = (params, opt_states, step_keys)\n",
    "\n",
    "    # Duplicate learner for update_batch_size.\n",
    "    broadcast = lambda x: jnp.broadcast_to(x, (config.system.update_batch_size,) + x.shape)\n",
    "    replicate_learner = jax.tree.map(broadcast, replicate_learner)\n",
    "\n",
    "    # Duplicate learner across devices.\n",
    "    replicate_learner = flax.jax_utils.replicate(replicate_learner, devices=jax.devices())\n",
    "\n",
    "    # Initialise learner state.\n",
    "    params, opt_states, step_keys = replicate_learner\n",
    "    init_learner_state = LearnerState(params, opt_states, step_keys, env_states, timesteps)\n",
    "\n",
    "    return learn, actor_network, init_learner_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GefUs8Yd7EJt"
   },
   "source": [
    "# Rendering and logging tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vkd95_VpYJf0"
   },
   "source": [
    "### Rendering\n",
    "The `render_one_episode` function simulates and visualises one episode from rolling out a trained MAPPO model that will be passed to the function using `actors_params`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "DU7OVSm6HM6q"
   },
   "outputs": [],
   "source": [
    "def render_one_episode(config, params, max_steps=100) -> None:\n",
    "    \"\"\"Rollout episodes of a trained MAPPO policy.\"\"\"\n",
    "    # Create envs\n",
    "    env_config = {**config.env.kwargs, **config.env.scenario.env_kwargs}\n",
    "    env, eval_env = env, eval_env = make_env()\n",
    "\n",
    "    # Create actor networks (We only care about the policy during rendering)\n",
    "    actor_network = Actor(env.action_dim)\n",
    "    apply_fn = actor_network.apply\n",
    "\n",
    "    reset_fn = jax.jit(env.reset)\n",
    "    step_fn = jax.jit(env.step)\n",
    "    key = jax.random.PRNGKey(config.system.seed)\n",
    "    key, reset_key = jax.random.split(key)\n",
    "    state, timestep = reset_fn(reset_key)\n",
    "\n",
    "    states = [state]\n",
    "    episode_return = 0\n",
    "    episode_length = 0\n",
    "    while not timestep.last():\n",
    "        key, action_key = jax.random.split(key)\n",
    "        pi = apply_fn(params, timestep.observation)\n",
    "\n",
    "        if config[\"arch\"][\"evaluation_greedy\"]:\n",
    "            action = pi.mode()\n",
    "        else:\n",
    "            action = pi.sample(seed=action_key)\n",
    "        state, timestep = step_fn(state, action)\n",
    "        states.append(state)\n",
    "        episode_return += jnp.mean(timestep.reward)\n",
    "        episode_length += 1\n",
    "\n",
    "    # Print out the results of the episode\n",
    "    print(f\"{Fore.CYAN}{Style.BRIGHT}EPISODE RETURN: {episode_return}{Style.RESET_ALL}\")\n",
    "    print(f\"{Fore.CYAN}{Style.BRIGHT}EPISODE LENGTH:{episode_length}{Style.RESET_ALL}\")\n",
    "\n",
    "    # Limit the number of steps to record to the maximum number of steps\n",
    "    steps = min([max_steps, len(states) - 1])\n",
    "    states = states[:steps]\n",
    "\n",
    "    # Render the episode\n",
    "    env.animate(states=states, interval=100, save_path=\"./applesauce.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2pDHF1Q8Cn7"
   },
   "source": [
    "### Logging:\n",
    "The `plot_performance` function visualises the performance of the algorithm. This plot will be refreshed each time evaluation interval happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "OwkZqb8y8GYG"
   },
   "outputs": [],
   "source": [
    "def plot_performance(mean_episode_return, ep_returns, start_time):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    # Plot the data\n",
    "    ep_returns.append(mean_episode_return)\n",
    "    plt.plot(\n",
    "        np.linspace(0, (time.time() - start_time) / 60.0, len(list(ep_returns))), list(ep_returns)\n",
    "    )\n",
    "    plt.xlabel(\"Run Time [Minutes]\")\n",
    "    plt.ylabel(\"Episode Return\")\n",
    "    plt.title(\"Apple Orchard with {x} Agents\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    return ep_returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GLLqQgn1754J"
   },
   "source": [
    "# Experiment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CwFYuKpfZyx9"
   },
   "source": [
    "The experiment setup includes: defining the hyperparameters, creating environments, setting up the learner and evaluator, and initialising some variables for plotting and logging purposes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHgQbXfY8LqF"
   },
   "source": [
    "#### Config\n",
    "\n",
    "We start the experiment setup by defining the config dictionary that represents a set of the various hyperparameters for the experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7LZsJW3K_xH"
   },
   "source": [
    "In addition to the typical hyperparameters used in MARL algorithms, we define below a few variables relevant to our implementation:\n",
    "\n",
    "`num_updates`: The number of gradient updates to perform during the training.\n",
    "\n",
    "`num_envs`:  Number of vectorised environments per device. For instance, if set to 512, it implies that 512 environments will be running in parallel at the same time on a given process.\n",
    "\n",
    "`num_evaluation` and `num_eval_episodes`: The `num_evaluation` parameter specifies how many evenly spaced evaluation steps will occur during training, while the `num_eval_episode` specifies how many episodes will be rolled out at each evaluation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "wexJ0Slr8INC"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"system\": {\n",
    "        \"actor_lr\": 2.5e-4,\n",
    "        \"critic_lr\": 2.5e-4,\n",
    "        \"update_batch_size\": 2, #1\n",
    "        \"rollout_length\": 128, #4\n",
    "        \"num_updates\": 200, #1\n",
    "        \"ppo_epochs\": 16, #1\n",
    "        \"num_minibatches\": 32, #1\n",
    "        \"gamma\": 0.99,\n",
    "        \"gae_lambda\": 0.95,\n",
    "        \"clip_eps\": 0.2,\n",
    "        \"ent_coef\": 0.01,\n",
    "        \"vf_coef\": 0.5,\n",
    "        \"max_grad_norm\": 0.5,\n",
    "        \"add_agent_id\": True,\n",
    "        \"decay_learning_rates\": False,\n",
    "        \"seed\": 42,\n",
    "    },\n",
    "    \"arch\": {\n",
    "        \"num_envs\": 32, #1\n",
    "        \"num_eval_episodes\": 32, #1\n",
    "        \"num_evaluation\": 50, #1\n",
    "        \"evaluation_greedy\": False,\n",
    "        \"num_absolute_metric_eval_episodes\": 32,\n",
    "    },\n",
    "    \"env\": {\n",
    "        \"eval_metric\": \"episode_return\",\n",
    "        \"implicit_agent_id\": False,\n",
    "        \"log_win_rate\": False,\n",
    "        \"kwargs\": {\"time_limit\": 500},\n",
    "        \"scenario\": {\n",
    "            \"task_config\": {\n",
    "                \"width\": 20,\n",
    "                \"height\": 15,\n",
    "                \"num_bots\": 2,\n",
    "                \"num_trees\": 5,\n",
    "                \"num_apples\": 30,\n",
    "                },\n",
    "            \"env_kwargs\": {},\n",
    "        },\n",
    "    },\n",
    "}\n",
    "# Convert the Python dictionary to a DictConfig\n",
    "config: DictConfig = OmegaConf.create(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sub4CAfrLHbM"
   },
   "source": [
    "#### Create the Training and Evaluation environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dwMHRotOLmdT"
   },
   "source": [
    "We use Mava's utility functions to create our environments for us. These environments will have a seuqnece of wrappers applied to them that will add agent identifiers and will log any relevant metrics. Since MAPPO has a centralised critic, we will also need the environment to return the true underlying environment state along with the individual agent observations. This is why we pass in `add_global_state=True`. FOr more information on all the wrappers that are applied, please see [here](https://github.com/instadeepai/Mava/blob/8b758133056e86303ab1acbe5aa2ade02e0f6e70/mava/utils/make_env.py#L86)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "H_mfilx6A7PQ"
   },
   "outputs": [],
   "source": [
    "# assign orchard name\n",
    "orchard_version_name = 'SimpleOrchard-v5'\n",
    "\n",
    "## register the orchard\n",
    "register(\n",
    "    id=orchard_version_name,\n",
    "    entry_point='__main__:SimpleOrchard',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "lCqZohi0vKSR"
   },
   "outputs": [],
   "source": [
    "# make the training and evaluation orchards\n",
    "env, eval_env = make_env(orchard_version_name, config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mCyGr8_bNurn"
   },
   "source": [
    "#### Additional variable definitions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZm3mf5uxo5m"
   },
   "source": [
    "In this section of the code, the total number of timesteps for the experiment is calculated, followed by the partitioning of the training timesteps into distinct intervals determined by the value of `num_evaluation`.\n",
    "\n",
    "**Calculating Total Timesteps:**\n",
    "\n",
    "To calculate the total timesteps, the following formula is used:\n",
    "```\n",
    "total_timesteps = n_devices\n",
    "* num_updates\n",
    "* rollout_length\n",
    "* update_batch_size\n",
    "* num_envs\n",
    "```\n",
    "- `n_devices` represents the number of JAX devices available, which is essential for parallel computation.\n",
    "- `num_updates` is the number of vectorised gradient updates to be be performed on each device.\n",
    "- `rollout_length` is the number of timesteps in each rollout.\n",
    "- `update_batch_size` is the batch size used for each update.\n",
    "- `num_envs` is the number of parallel environments used for data collection.\n",
    "\n",
    "This computation yields the total count of timesteps that will be carried out throughout the complete training procedure. Consequently, the number of timesteps within each training interval is established as ```total_timesteps/num_evaluation```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "XeqzRKVPxP2F"
   },
   "outputs": [],
   "source": [
    "def compute_total_timesteps(config: DictConfig):\n",
    "    # Calculate total timesteps.\n",
    "    n_devices = len(jax.devices())\n",
    "    config[\"system\"][\"num_updates_per_eval\"] = (\n",
    "        config[\"system\"][\"num_updates\"] // config[\"arch\"][\"num_evaluation\"]\n",
    "    )\n",
    "    steps_per_rollout = (\n",
    "        n_devices\n",
    "        * config[\"system\"][\"num_updates_per_eval\"]\n",
    "        * config[\"system\"][\"rollout_length\"]\n",
    "        * config[\"system\"][\"update_batch_size\"]\n",
    "        * config[\"arch\"][\"num_envs\"]\n",
    "    )\n",
    "\n",
    "    return steps_per_rollout, config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PrFx-V-DNUkN"
   },
   "source": [
    "#### The Learner and Evaluator Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "gp-FLgLSNg29"
   },
   "outputs": [],
   "source": [
    "# PRNG keys.\n",
    "key, key_e, actor_net_key, critic_net_key = jax.random.split(\n",
    "    jax.random.PRNGKey(config.system.seed), num=4\n",
    ")\n",
    "\n",
    "# Setup learner.\n",
    "learn, actor_network, learner_state = learner_setup(\n",
    "    env, (key, actor_net_key, critic_net_key), config\n",
    ")\n",
    "\n",
    "eval_act_fn = make_ff_eval_act_fn(actor_network.apply, config)\n",
    "\n",
    "# Setup evaluator.\n",
    "evaluator = get_eval_fn(eval_env, eval_act_fn, config, absolute_metric=False)\n",
    "absolute_metric_evaluator = get_eval_fn(eval_env, eval_act_fn, config, absolute_metric=True)\n",
    "\n",
    "# Add total timesteps to the config and compute environment steps per rollout.\n",
    "steps_per_rollout, config = compute_total_timesteps(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTMzsQEOa4Bv"
   },
   "source": [
    "# Run Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gPHBA2aU1jnC"
   },
   "source": [
    "#### Execute the experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SOMJZaDGbx8P"
   },
   "source": [
    "Now that the code has been compiled using JAX, its execution will benefit from optimised performance. We will proceed to train the MAPPO algorithm on the `small-4ag-easy` scenario from RobotWarehouse. The experiment follows a cyclic pattern, transitioning from training to evaluation and back to training.\n",
    "\n",
    "The training phase consists of performing 400 updates. Each update utilizes 512 parallel environments, with a rollout of 128 steps per environment and a batch of two vectorised full gradient update steps are performend. This comprehensive process results in over 50 million timesteps utilised for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "id": "ui6mWeXFFr4M",
    "outputId": "63b1ce7c-e02a-43ee-9904-5f61f3a952d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Entity distances: Traced<ShapedArray(int32[2])>with<BatchTrace(level=6/1)> with\n",
      "  val = Traced<ShapedArray(int32[2,2])>with<BatchTrace(level=5/1)> with\n",
      "    val = Traced<ShapedArray(int32[30,2,2])>with<BatchTrace(level=4/1)> with\n",
      "      val = Traced<ShapedArray(int32[32,30,2,2])>with<DynamicJaxprTrace(level=3/1)>\n",
      "      batch_dim = 0\n",
      "    batch_dim = 0\n",
      "  batch_dim = 0\n",
      "eaten_this_step: Traced<ShapedArray(bool[30])>with<BatchTrace(level=4/1)> with\n",
      "  val = Traced<ShapedArray(bool[32,30])>with<DynamicJaxprTrace(level=3/1)>\n",
      "  batch_dim = 0\n",
      "new_apples: SimpleOrchardApple(id=Traced<ShapedArray(int32[30])>with<BatchTrace(level=4/1)> with\n",
      "  val = Traced<ShapedArray(int32[32,30])>with<DynamicJaxprTrace(level=3/1)>\n",
      "  batch_dim = 0, position=Traced<ShapedArray(int32[30,2])>with<BatchTrace(level=4/1)> with\n",
      "  val = Traced<ShapedArray(int32[32,30,2])>with<DynamicJaxprTrace(level=3/1)>\n",
      "  batch_dim = 0, collected=Traced<ShapedArray(bool[30])>with<BatchTrace(level=4/1)> with\n",
      "  val = Traced<ShapedArray(bool[32,30])>with<DynamicJaxprTrace(level=3/1)>\n",
      "  batch_dim = 0)\n",
      "Transition: PPOTransition(done=Traced<ShapedArray(bool[32,2])>with<DynamicJaxprTrace(level=3/1)>, action=Traced<ShapedArray(int32[32,2])>with<DynamicJaxprTrace(level=3/1)>, value=Traced<ShapedArray(float32[32,2])>with<DynamicJaxprTrace(level=3/1)>, reward=Traced<ShapedArray(float32[32,2])>with<DynamicJaxprTrace(level=3/1)>, log_prob=Traced<ShapedArray(float32[32,2])>with<DynamicJaxprTrace(level=3/1)>, obs=SimpleOrchardObservation(agents_view=Traced<ShapedArray(int32[32,2,74])>with<DynamicJaxprTrace(level=3/1)>, action_mask=Traced<ShapedArray(bool[32,2,6])>with<DynamicJaxprTrace(level=3/1)>, time=Traced<ShapedArray(int32[32])>with<DynamicJaxprTrace(level=3/1)>), info={'episode_length': Traced<ShapedArray(int32[32])>with<DynamicJaxprTrace(level=3/1)>, 'episode_return': Traced<ShapedArray(float32[32])>with<DynamicJaxprTrace(level=3/1)>, 'is_terminal_step': Traced<ShapedArray(bool[32])>with<DynamicJaxprTrace(level=3/1)>})\n"
     ]
    }
   ],
   "source": [
    "# Run experiment for a total number of evaluations.\n",
    "ep_returns = []\n",
    "start_time = time.time()\n",
    "n_devices = len(jax.devices())\n",
    "\n",
    "# exploring code for a single evaluation\n",
    "# un-comment for multiple evaluations.\n",
    "for i in range(config[\"arch\"][\"num_evaluation\"]):\n",
    "    print(i)\n",
    "    # Train.\n",
    "    learner_output = learn(learner_state)\n",
    "    jax.block_until_ready(learner_output)\n",
    "\n",
    "    print('trained')\n",
    "    # collecting training data\n",
    "\n",
    "    # Prepare for evaluation.\n",
    "    trained_params = unreplicate_batch_dim(learner_state.params.actor_params)\n",
    "    print('unreplicated')\n",
    "\n",
    "    key_e, *eval_keys = jax.random.split(key_e, n_devices + 1)\n",
    "    eval_keys = jnp.stack(eval_keys)\n",
    "    eval_keys = eval_keys.reshape(n_devices, -1)\n",
    "\n",
    "    print('reshaped')\n",
    "\n",
    "    # Evaluate.\n",
    "    evaluator_output = evaluator(trained_params, eval_keys, {})\n",
    "    jax.block_until_ready(evaluator_output)\n",
    "\n",
    "    print('evaluated')\n",
    "\n",
    "    mean_episode_return = jnp.mean(evaluator_output[\"episode_return\"])\n",
    "    ep_returns = plot_performance(mean_episode_return, ep_returns, start_time)\n",
    "\n",
    "    print('plotted')\n",
    "\n",
    "    # Update runner state to continue training.\n",
    "    learner_state = learner_output.learner_state\n",
    "\n",
    "# Return trained params to be used for rendering or testing.\n",
    "trained_params = unreplicate_n_dims(trained_params, unreplicate_depth=1)\n",
    "\n",
    "print(f\"{Fore.CYAN}{Style.BRIGHT}MAPPO experiment completed{Style.RESET_ALL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1kTqQt2T_-Zb"
   },
   "source": [
    "# Review Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5u5OEEYgIMud",
    "outputId": "2e1f17c7-898f-487a-a59a-eee2f1ac83ad"
   },
   "outputs": [],
   "source": [
    "# extracting learner state, episode and train metrics\n",
    "learner_state, episode_metrics, train_metrics = learner_output\n",
    "# reviewing episode metrics\n",
    "episode_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ca9SZCs2JXZG",
    "outputId": "55f1cce6-218c-45dc-83c1-c86e9c391277"
   },
   "outputs": [],
   "source": [
    "# reviewing train metrics\n",
    "train_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ES_-uodaZMzg",
    "outputId": "3568557a-ba0f-4e26-ac65-e48b598f24aa"
   },
   "outputs": [],
   "source": [
    "learner_state.env_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "3-Zwu4D3-xr2",
    "outputId": "e28d0716-1210-42f5-e29c-4988d439eae4"
   },
   "outputs": [],
   "source": [
    "data = learner_state.timestep.extras\n",
    "\n",
    "def table_episode_metrics(data):\n",
    "  # Flattening the nested arrays\n",
    "  episode_length = data['episode_metrics']['episode_length'].flatten()\n",
    "  episode_return = data['episode_metrics']['episode_return'].flatten()\n",
    "  is_terminal_step = data['episode_metrics']['is_terminal_step'].flatten()\n",
    "  percent_eaten = data['percent_eaten'].flatten()\n",
    "\n",
    "  # Creating a dictionary for Pandas\n",
    "  flattened_data = {\n",
    "      'episode_length': episode_length,\n",
    "      'episode_return': episode_return,\n",
    "      'is_terminal_step': is_terminal_step,\n",
    "      'percent_eaten': percent_eaten\n",
    "  }\n",
    "\n",
    "  # Creating the DataFrame\n",
    "  df = pd.DataFrame(flattened_data)\n",
    "\n",
    "  # Display the DataFrame\n",
    "  return df\n",
    "\n",
    "table = table_episode_metrics(data)\n",
    "table.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xKXHT9Rh3BnR"
   },
   "source": [
    "#### Rendering **DOES NOT FUNCTION WITH SIMPLE ORCHARD**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y09oBv9cczeO"
   },
   "source": [
    "Now let's render one episode using the trained system\n",
    "\n",
    "> Note: Creating a complete episode animation can be time-consuming. To address this, we offer a parameter named `max_steps` for the `render_one_episode` function. This parameter determines the number of states displayed in the GIF. Please note that a full episode usually consists of 500 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "id": "lMSKw2_q8YHW",
    "outputId": "3a48a1b5-2839-43cd-c95d-c65d1371a405"
   },
   "outputs": [],
   "source": [
    "render_one_episode(config, trained_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PRV5gq1ZFr4S"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "if os.path.exists(\"/content/applesauce.gif\"):\n",
    "    display(Image(filename=\"/content/applesauce.gif\"))\n",
    "else:\n",
    "    display(Image(filename=\"./applesauce.gif\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vSFHb-_6OoMX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
