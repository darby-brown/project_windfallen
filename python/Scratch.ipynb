{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "831433df-ca17-4ba7-b272-196f83285b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d76be602-a043-4870-b09a-505678a404d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CudaDevice(id=0)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64b1f7c2-9daf-4e97-9726-cc76aef4558b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/dqn/#dqn_jaxpy\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "import gymnasium as gym\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "import tyro\n",
    "from flax.training.train_state import TrainState\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b531acc-e4da-444d-ac72-d92886345934",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Args:\n",
    "    exp_name: str = 'scratch'\n",
    "    \"\"\"the name of this experiment\"\"\"\n",
    "    seed: int = 1\n",
    "    \"\"\"seed of the experiment\"\"\"\n",
    "    track: bool = False\n",
    "    \"\"\"if toggled, this experiment will be tracked with Weights and Biases\"\"\"\n",
    "    wandb_project_name: str = \"cleanRL\"\n",
    "    \"\"\"the wandb's project name\"\"\"\n",
    "    wandb_entity: str = None\n",
    "    \"\"\"the entity (team) of wandb's project\"\"\"\n",
    "    capture_video: bool = False\n",
    "    \"\"\"whether to capture videos of the agent performances (check out `videos` folder)\"\"\"\n",
    "    save_model: bool = False\n",
    "    \"\"\"whether to save model into the `runs/{run_name}` folder\"\"\"\n",
    "    upload_model: bool = False\n",
    "    \"\"\"whether to upload the saved model to huggingface\"\"\"\n",
    "    hf_entity: str = \"\"\n",
    "    \"\"\"the user or org name of the model repository from the Hugging Face Hub\"\"\"\n",
    "\n",
    "    # Algorithm specific arguments\n",
    "    env_id: str = \"CartPole-v1\"\n",
    "    \"\"\"the id of the environment\"\"\"\n",
    "    total_timesteps: int = 500000\n",
    "    \"\"\"total timesteps of the experiments\"\"\"\n",
    "    learning_rate: float = 2.5e-4\n",
    "    \"\"\"the learning rate of the optimizer\"\"\"\n",
    "    num_envs: int = 1\n",
    "    \"\"\"the number of parallel game environments\"\"\"\n",
    "    buffer_size: int = 10000\n",
    "    \"\"\"the replay memory buffer size\"\"\"\n",
    "    gamma: float = 0.99\n",
    "    \"\"\"the discount factor gamma\"\"\"\n",
    "    tau: float = 1.0\n",
    "    \"\"\"the target network update rate\"\"\"\n",
    "    target_network_frequency: int = 500\n",
    "    \"\"\"the timesteps it takes to update the target network\"\"\"\n",
    "    batch_size: int = 128\n",
    "    \"\"\"the batch size of sample from the reply memory\"\"\"\n",
    "    start_e: float = 1\n",
    "    \"\"\"the starting epsilon for exploration\"\"\"\n",
    "    end_e: float = 0.05\n",
    "    \"\"\"the ending epsilon for exploration\"\"\"\n",
    "    exploration_fraction: float = 0.5\n",
    "    \"\"\"the fraction of `total-timesteps` it takes from start-e to go end-e\"\"\"\n",
    "    learning_starts: int = 10000\n",
    "    \"\"\"timestep to start learning\"\"\"\n",
    "    train_frequency: int = 10\n",
    "    \"\"\"the frequency of training\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19cbd01f-d413-4998-8bd6-805d5a175151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_id, seed, idx, capture_video, run_name):\n",
    "    def thunk():\n",
    "        if capture_video and idx == 0:\n",
    "            env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "            env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\")\n",
    "        else:\n",
    "            env = gym.make(env_id)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        env.action_space.seed(seed)\n",
    "\n",
    "        return env\n",
    "\n",
    "    return thunk\n",
    "\n",
    "\n",
    "# ALGO LOGIC: initialize agent here:\n",
    "class QNetwork(nn.Module):\n",
    "    action_dim: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: jnp.ndarray):\n",
    "        x = nn.Dense(120)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(84)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(self.action_dim)(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TrainState(TrainState):\n",
    "    target_params: flax.core.FrozenDict\n",
    "\n",
    "\n",
    "def linear_schedule(start_e: float, end_e: float, duration: int, t: int):\n",
    "    slope = (end_e - start_e) / duration\n",
    "    return max(slope * t + start_e, end_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83ce93f0-1677-4d9a-95ba-8e3460d4e064",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/river/.local/lib/python3.10/site-packages/tyro/_fields.py:181: UserWarning: The field wandb_entity is annotated with type <class 'str'>, but the default value None has type <class 'NoneType'>. We'll try to handle this gracefully, but it may cause unexpected behavior.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff0000; text-decoration-color: #ff0000\">╭─ </span><span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">Unrecognized options</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\"> ──────────────────────────────╮</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> Unrecognized options: -f                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">───────────────────────────────────────────────────</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> For full helptext, run <span style=\"font-weight: bold\">ipykernel_launcher.py --help</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">╰─────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[91m╭─\u001b[0m\u001b[91m \u001b[0m\u001b[1;91mUnrecognized options\u001b[0m\u001b[91m \u001b[0m\u001b[91m─────────────────────────────\u001b[0m\u001b[91m─╮\u001b[0m\n",
       "\u001b[91m│\u001b[0m Unrecognized options: -f                            \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[31m───────────────────────────────────────────────────\u001b[0m \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m For full helptext, run \u001b[1mipykernel_launcher.py --help\u001b[0m \u001b[91m│\u001b[0m\n",
       "\u001b[91m╰─────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/river/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import stable_baselines3 as sb3\n",
    "\n",
    "if sb3.__version__ < \"2.0\":\n",
    "    raise ValueError(\n",
    "        \"\"\"Ongoing migration: run the following command to install the new dependencies:\n",
    "\n",
    "poetry run pip install \"stable_baselines3==2.0.0a1\"\n",
    "\"\"\"\n",
    "    )\n",
    "args = tyro.cli(Args)\n",
    "assert args.num_envs == 1, \"vectorized envs are not supported at the moment\"\n",
    "run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
    "if args.track:\n",
    "    import wandb\n",
    "\n",
    "    wandb.init(\n",
    "        project=args.wandb_project_name,\n",
    "        entity=args.wandb_entity,\n",
    "        sync_tensorboard=True,\n",
    "        config=vars(args),\n",
    "        name=run_name,\n",
    "        monitor_gym=True,\n",
    "        save_code=True,\n",
    "    )\n",
    "writer = SummaryWriter(f\"runs/{run_name}\")\n",
    "writer.add_text(\n",
    "    \"hyperparameters\",\n",
    "    \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",
    ")\n",
    "\n",
    "# TRY NOT TO MODIFY: seeding\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "key = jax.random.PRNGKey(args.seed)\n",
    "key, q_key = jax.random.split(key, 2)\n",
    "\n",
    "# env setup\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [make_env(args.env_id, args.seed + i, i, args.capture_video, run_name) for i in range(args.num_envs)]\n",
    ")\n",
    "assert isinstance(envs.single_action_space, gym.spaces.Discrete), \"only discrete action space is supported\"\n",
    "\n",
    "obs, _ = envs.reset(seed=args.seed)\n",
    "q_network = QNetwork(action_dim=envs.single_action_space.n)\n",
    "q_state = TrainState.create(\n",
    "    apply_fn=q_network.apply,\n",
    "    params=q_network.init(q_key, obs),\n",
    "    target_params=q_network.init(q_key, obs),\n",
    "    tx=optax.adam(learning_rate=args.learning_rate),\n",
    ")\n",
    "\n",
    "q_network.apply = jax.jit(q_network.apply)\n",
    "# This step is not necessary as init called on same observation and key will always lead to same initializations\n",
    "q_state = q_state.replace(target_params=optax.incremental_update(q_state.params, q_state.target_params, 1))\n",
    "\n",
    "rb = ReplayBuffer(\n",
    "    args.buffer_size,\n",
    "    envs.single_observation_space,\n",
    "    envs.single_action_space,\n",
    "    \"cpu\",\n",
    "    handle_timeout_termination=False,\n",
    ")\n",
    "\n",
    "@jax.jit\n",
    "def update(q_state, observations, actions, next_observations, rewards, dones):\n",
    "    q_next_target = q_network.apply(q_state.target_params, next_observations)  # (batch_size, num_actions)\n",
    "    q_next_target = jnp.max(q_next_target, axis=-1)  # (batch_size,)\n",
    "    next_q_value = rewards + (1 - dones) * args.gamma * q_next_target\n",
    "\n",
    "    def mse_loss(params):\n",
    "        q_pred = q_network.apply(params, observations)  # (batch_size, num_actions)\n",
    "        q_pred = q_pred[jnp.arange(q_pred.shape[0]), actions.squeeze()]  # (batch_size,)\n",
    "        return ((q_pred - next_q_value) ** 2).mean(), q_pred\n",
    "\n",
    "    (loss_value, q_pred), grads = jax.value_and_grad(mse_loss, has_aux=True)(q_state.params)\n",
    "    q_state = q_state.apply_gradients(grads=grads)\n",
    "    return loss_value, q_pred, q_state\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# TRY NOT TO MODIFY: start the game\n",
    "obs, _ = envs.reset(seed=args.seed)\n",
    "for global_step in range(args.total_timesteps):\n",
    "    # ALGO LOGIC: put action logic here\n",
    "    epsilon = linear_schedule(args.start_e, args.end_e, args.exploration_fraction * args.total_timesteps, global_step)\n",
    "    if random.random() < epsilon:\n",
    "        actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])\n",
    "    else:\n",
    "        q_values = q_network.apply(q_state.params, obs)\n",
    "        actions = q_values.argmax(axis=-1)\n",
    "        actions = jax.device_get(actions)\n",
    "\n",
    "    # TRY NOT TO MODIFY: execute the game and log data.\n",
    "    next_obs, rewards, terminations, truncations, infos = envs.step(actions)\n",
    "\n",
    "    # TRY NOT TO MODIFY: record rewards for plotting purposes\n",
    "    if \"final_info\" in infos:\n",
    "        for info in infos[\"final_info\"]:\n",
    "            if info and \"episode\" in info:\n",
    "                print(f\"global_step={global_step}, episodic_return={info['episode']['r']}\")\n",
    "                writer.add_scalar(\"charts/episodic_return\", info[\"episode\"][\"r\"], global_step)\n",
    "                writer.add_scalar(\"charts/episodic_length\", info[\"episode\"][\"l\"], global_step)\n",
    "\n",
    "    # TRY NOT TO MODIFY: save data to reply buffer; handle `final_observation`\n",
    "    real_next_obs = next_obs.copy()\n",
    "    for idx, trunc in enumerate(truncations):\n",
    "        if trunc:\n",
    "            real_next_obs[idx] = infos[\"final_observation\"][idx]\n",
    "    rb.add(obs, real_next_obs, actions, rewards, terminations, infos)\n",
    "\n",
    "    # TRY NOT TO MODIFY: CRUCIAL step easy to overlook\n",
    "    obs = next_obs\n",
    "\n",
    "    # ALGO LOGIC: training.\n",
    "    if global_step > args.learning_starts:\n",
    "        if global_step % args.train_frequency == 0:\n",
    "            data = rb.sample(args.batch_size)\n",
    "            # perform a gradient-descent step\n",
    "            loss, old_val, q_state = update(\n",
    "                q_state,\n",
    "                data.observations.numpy(),\n",
    "                data.actions.numpy(),\n",
    "                data.next_observations.numpy(),\n",
    "                data.rewards.flatten().numpy(),\n",
    "                data.dones.flatten().numpy(),\n",
    "            )\n",
    "\n",
    "            if global_step % 100 == 0:\n",
    "                writer.add_scalar(\"losses/td_loss\", jax.device_get(loss), global_step)\n",
    "                writer.add_scalar(\"losses/q_values\", jax.device_get(old_val).mean(), global_step)\n",
    "                print(\"SPS:\", int(global_step / (time.time() - start_time)))\n",
    "                writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "\n",
    "        # update target network\n",
    "        if global_step % args.target_network_frequency == 0:\n",
    "            q_state = q_state.replace(\n",
    "                target_params=optax.incremental_update(q_state.params, q_state.target_params, args.tau)\n",
    "            )\n",
    "\n",
    "if args.save_model:\n",
    "    model_path = f\"runs/{run_name}/{args.exp_name}.cleanrl_model\"\n",
    "    with open(model_path, \"wb\") as f:\n",
    "        f.write(flax.serialization.to_bytes(q_state.params))\n",
    "    print(f\"model saved to {model_path}\")\n",
    "    from cleanrl_utils.evals.dqn_jax_eval import evaluate\n",
    "\n",
    "    episodic_returns = evaluate(\n",
    "        model_path,\n",
    "        make_env,\n",
    "        args.env_id,\n",
    "        eval_episodes=10,\n",
    "        run_name=f\"{run_name}-eval\",\n",
    "        Model=QNetwork,\n",
    "        epsilon=0.05,\n",
    "    )\n",
    "    for idx, episodic_return in enumerate(episodic_returns):\n",
    "        writer.add_scalar(\"eval/episodic_return\", episodic_return, idx)\n",
    "\n",
    "    if args.upload_model:\n",
    "        from cleanrl_utils.huggingface import push_to_hub\n",
    "\n",
    "        repo_name = f\"{args.env_id}-{args.exp_name}-seed{args.seed}\"\n",
    "        repo_id = f\"{args.hf_entity}/{repo_name}\" if args.hf_entity else repo_name\n",
    "        push_to_hub(args, episodic_returns, repo_id, \"DQN\", f\"runs/{run_name}\", f\"videos/{run_name}-eval\")\n",
    "\n",
    "envs.close()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e17b96a-41e3-40b7-90bd-fb15858f5511",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
